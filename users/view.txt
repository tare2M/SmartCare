from django.shortcuts import render
from django.contrib.auth.decorators import login_required
from django.contrib.auth import login, logout
from django.shortcuts import render, redirect
from django.contrib.auth.decorators import login_required
from django.contrib.auth.forms import AuthenticationForm
from .forms import DoctorRegistrationForm, PatientRegistrationForm,UserLoginForm,AppointmentForm
from django.shortcuts import render
from .models import Doctor, Patient,Appointment
from django.contrib.auth import login, authenticate
from django.shortcuts import render, redirect
from django.contrib import messages
from django.http import JsonResponse
#from .chat import get_response  # Import the chatbot function
import random
import json
import torch
import numpy as np
import nltk
from nltk.stem.porter import PorterStemmer
import json
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
'''from .model1 import NeuralNet
from .nltk_utils import bag_of_words, tokenize, stem'''

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_sizes, num_classes):
        super(NeuralNet, self).__init__()

        # Create a list to hold the hidden layers
        self.hidden_layers = nn.ModuleList()

        # Add input layer
        self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[0]))

        # Add hidden layers
        for i in range(len(hidden_sizes) - 1):
            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))

        # Add output layer
        self.output_layer = nn.Linear(hidden_sizes[-1], num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        # Forward pass through the network
        for layer in self.hidden_layers:
            x = self.relu(layer(x))
        out = self.output_layer(x)

        return out

# Example usage of the NeuralNet class
if __name__ == "__main__":
    # Define input size, hidden layer sizes, and output size
    input_size = 10
    hidden_sizes = [20, 30, 40, 30, 20]  # Modify this list to set the sizes of each hidden layer
    num_classes = 5

    # Create an instance of the neural network
    model = NeuralNet(input_size, hidden_sizes, num_classes)

    # Print the model architecture
    #print(model)
class NltkUtils:
    def __init__(self):
        # Initialize the Porter Stemmer
        self.stemmer = PorterStemmer()

    def tokenize(self, sentence):
        """
        Tokenize a sentence into words.

        Args:
            sentence (str): The input sentence.

        Returns:
            list: A list of tokens (words).
        """
        return nltk.word_tokenize(sentence)

    def stem(self, word):
        """
        Perform stemming on a word.

        Args:
            word (str): The word to be stemmed.

        Returns:
            str: The stemmed word in lowercase.
        """
        return self.stemmer.stem(word.lower())

    def bag_of_words(self, tokenized_sentence, words):
        """
        Create a bag of words representation for a tokenized sentence.

        Args:
            tokenized_sentence (list): List of tokens in a sentence.
            words (list): List of known words in the vocabulary.

        Returns:
            numpy.ndarray: A binary array representing the presence of words in the sentence.
        """
        # Stem each word in the sentence
        sentence_words = [self.stem(word) for word in tokenized_sentence]

        # Initialize a bag with 0 for each word
        bag = np.zeros(len(words), dtype=np.float32)

        for idx, w in enumerate(words):
            if w in sentence_words:
                bag[idx] = 1

        return bag
class ChatBotTrainer:
    def __init__(self, intents_file='intents.json'):
        self.intents_file = intents_file
        self.all_words = []
        self.tags = []
        self.xy = []

        # Define hyperparameters
        self.input_size = 0
        self.hidden_sizes = [64, 128, 128, 64, 32]  # Modify this list to set the sizes of each hidden layer
        self.output_size = 0
        self.num_epochs = 1000
        self.batch_size = 8
        self.learning_rate = 0.001

    def prepare_data(self):
        with open('intents.json', 'r', encoding="UTF8") as f:
            intents = json.load(f)

        for intent in intents['intents']:
            tag = intent['tag']
            self.tags.append(tag)

            for pattern in intent['patterns']:
                words = tokenize(pattern)
                self.all_words.extend(words)
                self.xy.append((words, tag))

        ignore_words = ['?', '.', '!']
        self.all_words = [stem(w) for w in self.all_words if w not in ignore_words]
        self.all_words = sorted(set(self.all_words))
        self.tags = sorted(set(self.tags))

    def prepare_training_data(self):
        X_train = []
        y_train = []

        for (pattern_sentence, tag) in self.xy:
            bag = bag_of_words(pattern_sentence, self.all_words)
            X_train.append(bag)
            label = self.tags.index(tag)
            y_train.append(label)

        X_train = np.array(X_train)
        y_train = np.array(y_train)

        self.input_size = len(X_train[0])
        self.output_size = len(self.tags)

        return X_train, y_train

    def train_model(self, model_file="data.pth"):
        self.prepare_data()
        X_train, y_train = self.prepare_training_data()

        # Create a custom dataset
        class ChatDataset(Dataset):
            def __init__(self, X, y):
                self.n_samples = len(X)
                self.x_data = X
                self.y_data = y

            def __getitem__(self, index):
                return self.x_data[index], self.y_data[index]

            def __len__(self):
                return self.n_samples

        dataset = ChatDataset(X_train, y_train)
        train_loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        model = NeuralNet(self.input_size, self.hidden_sizes, self.output_size).to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)

        for epoch in range(self.num_epochs):
            for (words, labels) in train_loader:
                words = words.to(device)
                labels = labels.to(dtype=torch.long).to(device)
                outputs = model(words)
                loss = criterion(outputs, labels)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            if (epoch + 1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.4f}')

        data = {
            "model_state": model.state_dict(),
            "input_size": self.input_size,
            "hidden_sizes": self.hidden_sizes,
            "output_size": self.output_size,
            "all_words": self.all_words,
            "tags": self.tags
        }

        FILE = model_file
        torch.save(data, FILE)

        print(f'Training complete. Model saved to {FILE}')
class ChatBot:
    def __init__(self):
        # Define the device (CPU or CUDA if available)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model, self.intents, self.all_words, self.tags = self.load_chatbot_data()
        self.bot_name = "SmartCare"
        self.nltk_utils = NltkUtils()

    def load_chatbot_data(self):
        # Load the intents data from a JSON file
        with open('intents.json', 'r', encoding="utf8") as json_data:
            intents = json.load(json_data)

        # Load the chatbot model and its configuration
        MODEL_FILE = "data.pth"
        model_data = torch.load(MODEL_FILE)

        input_size = model_data["input_size"]
        hidden_sizes = model_data["hidden_sizes"]
        output_size = model_data["output_size"]
        all_words = model_data['all_words']
        tags = model_data['tags']
        model_state = model_data["model_state"]

        # Initialize the model and set it to evaluation mode
        model = NeuralNet(input_size, hidden_sizes, output_size).to(self.device)
        model.load_state_dict(model_state)
        model.eval()

        return model, intents, all_words, tags

    def get_response(self, msg):
        # Ensure that the message is a string
        if isinstance(msg, str):
            # Tokenize the user's message
            sentence = self.nltk_utils.tokenize(msg)
            # Convert the message to a bag of words
            X = self.nltk_utils.bag_of_words(sentence, self.all_words)
            X = X.reshape(1, X.shape[0])
            X = torch.from_numpy(X).to(self.device)

            # Get the model's output
            output = self.model(X)
            _, predicted = torch.max(output, dim=1)

            tag = self.tags[predicted.item()]

            probs = torch.softmax(output, dim=1)
            prob = probs[0][predicted.item()]

            if prob.item() > 0.75:
                for intent in self.intents['intents']:
                    if tag == intent["tag"]:
                        return random.choice(intent['responses'])

            return "أعطني معلومات أوضح من فضلك"

if __name__ == "__main__":
    chatbot = ChatBot()
    print("Let's chat! (type 'quit' to exit)")
    while True:
        sentence = input("You: ")
        if sentence == "quit":
            break

        resp = chatbot.get_response(sentence)
        print(f"{chatbot.bot_name}: {resp}")



def chatbot_view(request):
    if request.method == 'POST':
        user_message = request.POST.get('user_message')
        chatbot = ChatBot()  # Initialize your chatbot
        chatbot_response_text = chatbot.get_response(user_message)
        return JsonResponse({'response': chatbot_response_text})
    else:
        return render(request, 'chatbot.html')

def home(request):
    return render(request, 'home.html')

def home(request):
    return render(request, 'home.html')

def doctor_signup(request):
    if request.method == 'POST':
        form = DoctorRegistrationForm(request.POST)
        if form.is_valid():
            user = form.save(commit=False)
            user.is_doctor = True
            user.save()
            # Create and link a Doctor object
            doctor = Doctor(user=user)
            doctor.save()
            # Log the user in
            login(request, user)
            return redirect('doctor_profile')
    else:
        form = DoctorRegistrationForm()
    return render(request, 'doctor/doctor_signup.html', {'form': form})

def patient_signup(request):
    if request.method == 'POST':
        form = PatientRegistrationForm(request.POST)
        if form.is_valid():
            user = form.save(commit=False)
            user.is_patient = True
            user.save()

            # Extract and save date_of_birth
            date_of_birth = form.cleaned_data['date_of_birth']

            # Create a Patient object with the date_of_birth
            patient = Patient(user=user, date_of_birth=date_of_birth)
            patient.save()

            # Log the user in
            login(request, user)
            return redirect('patient_profile')
    else:
        form = PatientRegistrationForm()
    return render(request, 'patient/patient_signup.html', {'form': form})


@login_required
def doctor_profile(request):
    try:
        doctor = Doctor.objects.get(user=request.user)
    except Doctor.DoesNotExist:
        # Handle the case where a Doctor object does not exist
        # Redirect the user to an appropriate page or show an error message
        return render(request, 'doctor/doctor_not_found.html')

    return render(request, 'doctor/doctor_profile.html', {'doctor': doctor})

@login_required
def patient_profile(request):
    try:
        patient = Patient.objects.get(user=request.user)
    except Patient.DoesNotExist:
        # Handle the case where a patient object does not exist
        # Redirect the user to an appropriate page or show an error message
        return render(request, 'patient/patient_not_found.html')

    return render(request, 'patient/patient_profile.html', {'patient': patient})

def user_login(request):
    if request.method == 'POST':
        form = UserLoginForm(request.POST)
        if form.is_valid():
            username = form.cleaned_data['username']
            password = form.cleaned_data['password']
            role = form.cleaned_data['role']

            user = authenticate(request, username=username, password=password)
            if user is not None:
                login(request, user)
                if role == 'patient':
                    return redirect('patient_profile')
                elif role == 'doctor':
                    return redirect('doctor_profile')
            else:
                # Handle invalid login
                pass
    else:
        form = UserLoginForm()
    return render(request, 'login.html', {'form': form})
@login_required
def user_logout(request):
    logout(request)
    return redirect('home')


@login_required
def book_appointment(request):
    if request.method == 'POST':
        form = AppointmentForm(request.POST)
        if form.is_valid():
            appointment = form.save(commit=False)
            appointment.patient = request.user.patient  # Assuming you have a patient profile linked to users
            appointment.save()
            messages.success(request, 'Appointment booked successfully.')
            return redirect('patient/patient_profile.html', {'patient': patient})
    else:
        form = AppointmentForm()
    return render(request, 'patient/book_appointment.html', {'form': form})

@login_required
def patient_appointments(request):
    appointments = Appointment.objects.filter(patient=request.user.patient)
    return render(request, 'patient/patient_profile.html', {'appointments': appointments})

@login_required
def doctor_schedule(request):
    if request.user.is_doctor:
        appointments = Appointment.objects.filter(doctor=request.user.doctor)
        return render(request, 'doctor/doctor_profile.html', {'appointments': appointments})
    else:
        return render(request, 'error.html', {'message': 'You are not authorized to access this page.'})




///////////////////////////////////////*\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\







'''#from .chat import get_response  # Import the chatbot function
import random
import json
import torch
import numpy as np
import nltk
from nltk.stem.porter import PorterStemmer
import json
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
#from .model1 import NeuralNet
#from .nltk_utils import bag_of_words, tokenize, stem

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_sizes, num_classes):
        super(NeuralNet, self).__init__()

        # Create a list to hold the hidden layers
        self.hidden_layers = nn.ModuleList()

        # Add input layer
        self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[0]))

        # Add hidden layers
        for i in range(len(hidden_sizes) - 1):
            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))

        # Add output layer
        self.output_layer = nn.Linear(hidden_sizes[-1], num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        # Forward pass through the network
        for layer in self.hidden_layers:
            x = self.relu(layer(x))
        out = self.output_layer(x)

        return out

# Example usage of the NeuralNet class
if __name__ == "__main__":
    # Define input size, hidden layer sizes, and output size
    input_size = 10
    hidden_sizes = [20, 30, 40, 30, 20]  # Modify this list to set the sizes of each hidden layer
    num_classes = 5

    # Create an instance of the neural network
    model = NeuralNet(input_size, hidden_sizes, num_classes)

    # Print the model architecture
    #print(model)
class NltkUtils:
    def __init__(self):
        # Initialize the Porter Stemmer
        self.stemmer = PorterStemmer()

    def tokenize(self, sentence):
        """
        Tokenize a sentence into words.

        Args:
            sentence (str): The input sentence.

        Returns:
            list: A list of tokens (words).
        """
        return nltk.word_tokenize(sentence)

    def stem(self, word):
        """
        Perform stemming on a word.

        Args:
            word (str): The word to be stemmed.

        Returns:
            str: The stemmed word in lowercase.
        """
        return self.stemmer.stem(word.lower())

    def bag_of_words(self, tokenized_sentence, words):
        """
        Create a bag of words representation for a tokenized sentence.

        Args:
            tokenized_sentence (list): List of tokens in a sentence.
            words (list): List of known words in the vocabulary.

        Returns:
            numpy.ndarray: A binary array representing the presence of words in the sentence.
        """
        # Stem each word in the sentence
        sentence_words = [self.stem(word) for word in tokenized_sentence]

        # Initialize a bag with 0 for each word
        bag = np.zeros(len(words), dtype=np.float32)

        for idx, w in enumerate(words):
            if w in sentence_words:
                bag[idx] = 1

        return bag
class ChatBotTrainer:
    def __init__(self, intents_file='intents.json'):
        self.intents_file = intents_file
        self.all_words = []
        self.tags = []
        self.xy = []

        # Define hyperparameters
        self.input_size = 0
        self.hidden_sizes = [64, 128, 128, 64, 32]  # Modify this list to set the sizes of each hidden layer
        self.output_size = 0
        self.num_epochs = 1000
        self.batch_size = 8
        self.learning_rate = 0.001

    def prepare_data(self):
        with open('intents.json', 'r', encoding="UTF8") as f:
            intents = json.load(f)

        for intent in intents['intents']:
            tag = intent['tag']
            self.tags.append(tag)

            for pattern in intent['patterns']:
                words = tokenize(pattern)
                self.all_words.extend(words)
                self.xy.append((words, tag))

        ignore_words = ['?', '.', '!']
        self.all_words = [stem(w) for w in self.all_words if w not in ignore_words]
        self.all_words = sorted(set(self.all_words))
        self.tags = sorted(set(self.tags))

    def prepare_training_data(self):
        X_train = []
        y_train = []

        for (pattern_sentence, tag) in self.xy:
            bag = bag_of_words(pattern_sentence, self.all_words)
            X_train.append(bag)
            label = self.tags.index(tag)
            y_train.append(label)

        X_train = np.array(X_train)
        y_train = np.array(y_train)

        self.input_size = len(X_train[0])
        self.output_size = len(self.tags)

        return X_train, y_train

    def train_model(self, model_file="data.pth"):
        self.prepare_data()
        X_train, y_train = self.prepare_training_data()

        # Create a custom dataset
        class ChatDataset(Dataset):
            def __init__(self, X, y):
                self.n_samples = len(X)
                self.x_data = X
                self.y_data = y

            def __getitem__(self, index):
                return self.x_data[index], self.y_data[index]

            def __len__(self):
                return self.n_samples

        dataset = ChatDataset(X_train, y_train)
        train_loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        model = NeuralNet(self.input_size, self.hidden_sizes, self.output_size).to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)

        for epoch in range(self.num_epochs):
            for (words, labels) in train_loader:
                words = words.to(device)
                labels = labels.to(dtype=torch.long).to(device)
                outputs = model(words)
                loss = criterion(outputs, labels)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            if (epoch + 1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.4f}')

        data = {
            "model_state": model.state_dict(),
            "input_size": self.input_size,
            "hidden_sizes": self.hidden_sizes,
            "output_size": self.output_size,
            "all_words": self.all_words,
            "tags": self.tags
        }

        FILE = model_file
        torch.save(data, FILE)

        print(f'Training complete. Model saved to {FILE}')
class ChatBot:
    def __init__(self):
        # Define the device (CPU or CUDA if available)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model, self.intents, self.all_words, self.tags = self.load_chatbot_data()
        self.bot_name = "SmartCare"
        self.nltk_utils = NltkUtils()

    def load_chatbot_data(self):
        # Load the intents data from a JSON file
        with open('intents.json', 'r', encoding="utf8") as json_data:
            intents = json.load(json_data)

        # Load the chatbot model and its configuration
        MODEL_FILE = "data.pth"
        model_data = torch.load(MODEL_FILE)

        input_size = model_data["input_size"]
        hidden_sizes = model_data["hidden_sizes"]
        output_size = model_data["output_size"]
        all_words = model_data['all_words']
        tags = model_data['tags']
        model_state = model_data["model_state"]

        # Initialize the model and set it to evaluation mode
        model = NeuralNet(input_size, hidden_sizes, output_size).to(self.device)
        model.load_state_dict(model_state)
        model.eval()

        return model, intents, all_words, tags

    def get_response(self, msg):
        # Ensure that the message is a string
        if isinstance(msg, str):
            # Tokenize the user's message
            sentence = self.nltk_utils.tokenize(msg)
            # Convert the message to a bag of words
            X = self.nltk_utils.bag_of_words(sentence, self.all_words)
            X = X.reshape(1, X.shape[0])
            X = torch.from_numpy(X).to(self.device)

            # Get the model's output
            output = self.model(X)
            _, predicted = torch.max(output, dim=1)

            tag = self.tags[predicted.item()]

            probs = torch.softmax(output, dim=1)
            prob = probs[0][predicted.item()]

            if prob.item() > 0.75:
                for intent in self.intents['intents']:
                    if tag == intent["tag"]:
                        return random.choice(intent['responses'])

            return "أعطني معلومات أوضح من فضلك"

if __name__ == "__main__":
    chatbot = ChatBot()
    print("Let's chat! (type 'quit' to exit)")
    while True:
        sentence = input("You: ")
        if sentence == "quit":
            break

        resp = chatbot.get_response(sentence)
        print(f"{chatbot.bot_name}: {resp}")

'''

'''def chatbot_view(request):
    if request.method == 'POST':
        user_message = request.POST.get('user_message')
        chatbot = ChatBot()  # Initialize your chatbot
        chatbot_response_text = chatbot.get_response(user_message)
        return JsonResponse({'response': chatbot_response_text})
    else:
        return render(request, 'chatbot.html')'''